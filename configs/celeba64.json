{
    "name": "celeba64", // experiments name
    "gpu_ids": [0], // gpu ids list, default is single 0
    "seed" : -1, // random seed, seed <0 represents randomization not used 
    "finetune_norm": false, // find the parameters to optimize

    "path": { //set every part file path
        "base_dir": "experiments", // base path for all log except resume_state
        "code": "code", // code backup
        "tb_logger": "tb_logger", // path of tensorboard logger
        "results": "results",
        "checkpoint": "checkpoint",
        "resume_state": null
        // "resume_state": null // ex: 100, loading .state  and .pth from given epoch and iteration
    },

    "datasets": { // train or test
        "train": { 
            "which_dataset": {  // import designated dataset using arguments 
                "name": "CelebAUnconditional", // import Dataset() class / function(not recommend) from data.dataset.py (default is [data.dataset.py])
                "args":{ // arguments to initialize dataset
                    "data_root": "datasets/",
                    "data_len": -1
                }
            },
            "dataloader":{
                "args":{ // arguments to initialize train_dataloader
                    "batch_size": 64, // batch size in each gpu
                    "num_workers": 40,
                    "shuffle": true,
                    "pin_memory": true,
                    "drop_last": true
                }
            }
        },
        "validation": {
            "which_dataset": {  // import designated dataset using arguments
                "name": "DirDataset", // import Dataset() class / function(not recommend) from data.dataset.py (default is [data.dataset.py])
                "args":{ // arguments to initialize dataset
                    "data_root": "datasets/celeba_val/",
                    "data_len": -1
                }
            },
            "dataloader":{
                "args":{ // arguments to initialize train_dataloader
                    "batch_size": 4, // batch size in each gpu
                    "num_workers": 0,
                    "shuffle": false,
                    "pin_memory": true,
                    "drop_last": false
                }
            }
        }
    },

    "model": { // networks/metrics/losses/optimizers/lr_schedulers is a list and model is a dict
        "trainer": { // import designated  model(trainer) using arguments
            "name": ["models.model", "Trainer"], // import Model() class / function(not recommend) from models.model.py (default is [models.model.py])
            "args": {
                "ema_scheduler": {
                    "ema_start": 1,
                    "ema_iter": 1,
                    "ema_decay": 0.9999
                },
                "optimizers": [
                    { "lr": 1e-6, "weight_decay": 0, "betas": [0.5, 0.999]}
                ],
                "mode": "normal"
            }
        }, 
        "network": // import designated list of networks using arguments
        {
            "name": ["models.ddim.diffusion", "Model"], // import Network() class / function(not recommend) from default file (default is [models/network.py])
            "args": {
                "channels": 128,
                "in_channels":  3,
                "out_channels":  3,
                "ch_mult":  [1, 2, 2, 2, 4],
                "num_res_blocks":  2,
                "attn_resolutions":  [16],
                "dropout":  0.1,
                "image_size":  64,
                "resamp_with_conv": true
            }
        },
        "discriminator": // import designated list of networks using arguments
        {
            "name": ["models.ddim.diffusion", "Enc"], // import Network() class / function(not recommend) from default file (default is [models/network.py])
            "args": {
                "channels": 128,
                "in_channels":  6,
                "ch_mult":  [1, 2, 2, 2, 4],
                "num_res_blocks":  2,
                "attn_resolutions":  [16],
                "dropout":  0.1,
                "image_size":  64,
                "resamp_with_conv": true
            }
        },
        "diffusion": {
            "beta_schedule": {
                        "beta_schedule": "linear",
                        "beta_start": 0.0001,
                        "beta_end": 0.02,
                        "num_diffusion_timesteps": 1000
                    }
        },
        "losses": [ // import designated list of losses without arguments
            "mse_loss" // import mse_loss() function/class from default file (default is [models/losses.py]), equivalent to { "name": "mse_loss", "args":{}}
        ],
        "metrics": [ // import designated list of metrics without arguments
            "mae" // import mae() function/class from default file (default is [models/metrics.py]), equivalent to { "name": "mae", "args":{}}
        ]
    },

    "train": { // arguments for basic training
        "n_epoch": 6e3, // max epochs, not limited now
        "n_iter": 1e8, // max interations
        "val_epoch": 1, // valdation every specified number of epochs
        "save_checkpoint_epoch": 10,
        "log_iter": 1e2, // log every specified number of iterations
        "tensorboard" : true // tensorboardX enable
    }
}
